{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pyarallel","text":"<p>A powerful, feature-rich parallel execution library for Python that makes concurrent programming easy and efficient.</p>"},{"location":"#overview","title":"Overview","text":"<p>Pyarallel simplifies parallel processing in Python by providing a decorator-based API that handles both I/O-bound and CPU-bound tasks efficiently. With features like automatic worker pool management, rate limiting, and batch processing, it's designed to make concurrent programming accessible while maintaining high performance.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Simple Decorator-Based API: Just add <code>@parallel</code> to your functions</li> <li>Flexible Parallelism: Choose between threads (I/O-bound) and processes (CPU-bound)</li> <li>Smart Rate Limiting: Control execution rates with per-second, per-minute, or per-hour limits</li> <li>Batch Processing: Handle large datasets efficiently with automatic batching</li> <li>Performance Optimized: </li> <li>Automatic worker pool reuse</li> <li>Optional worker prewarming for latency-critical applications</li> <li>Smart defaults based on your system</li> <li>Production Ready:</li> <li>Thread-safe implementation</li> <li>Memory-efficient with automatic cleanup</li> <li>Comprehensive error handling</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from pyarallel import parallel\n\n# Basic parallel processing\n@parallel(max_workers=4)\ndef fetch_url(url: str) -&gt; dict:\n    return requests.get(url).json()\n\n# Process multiple URLs in parallel\nurls = [\"http://api1.com\", \"http://api2.com\"]\nresults = fetch_url(urls)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyarallel\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore our comprehensive documentation to learn more:</p> <ul> <li>Quick Start Guide</li> <li>Basic Usage</li> <li>Advanced Features</li> <li>Configuration</li> <li>Best Practices</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Check out our Contributing Guide to get started.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"api-reference/configuration-api/","title":"API Reference","text":""},{"location":"api-reference/configuration-api/#configuration-api","title":"Configuration API","text":""},{"location":"api-reference/configuration-api/#configmanager","title":"ConfigManager","text":"<p>Singleton class for managing global configuration.</p> <pre><code>from pyarallel import ConfigManager\n\nconfig_manager = ConfigManager.get_instance()\n</code></pre>"},{"location":"api-reference/configuration-api/#methods","title":"Methods","text":"<ul> <li><code>get_config()</code>: Get current configuration</li> <li><code>update_config(config: dict)</code>: Update configuration with new values</li> <li><code>reset_config()</code>: Reset to default configuration</li> </ul>"},{"location":"api-reference/configuration-api/#configuration-options","title":"Configuration Options","text":"<pre><code>{\n    \"execution\": {\n        \"default_max_workers\": 4,\n        \"default_executor_type\": \"thread\",\n        \"default_batch_size\": 10\n    }\n}\n</code></pre>"},{"location":"api-reference/configuration-api/#examples","title":"Examples","text":"<pre><code># Get config manager instance\nconfig_manager = ConfigManager.get_instance()\n\n# Update global configuration\nconfig_manager.update_config({\n    \"execution\": {\n        \"default_max_workers\": 8,\n        \"default_executor_type\": \"process\"\n    }\n})\n\n# Get current configuration\nconfig = config_manager.get_config()\n</code></pre>"},{"location":"api-reference/decorators/","title":"API Reference","text":""},{"location":"api-reference/decorators/#decorator-api","title":"Decorator API","text":""},{"location":"api-reference/decorators/#parallel","title":"@parallel","text":"<p>The main decorator for enabling parallel execution of functions.</p> <pre><code>from pyarallel import parallel\n\n@parallel(\n    max_workers: int = None,\n    batch_size: int = None,\n    rate_limit: float | tuple[float, TimeUnit] | RateLimit = None,\n    executor_type: Literal[\"thread\", \"process\"] = None,\n    prewarm: bool = False\n)\ndef function(item, *args, **kwargs) -&gt; Any: ...\n</code></pre>"},{"location":"api-reference/decorators/#parameters","title":"Parameters","text":"<ul> <li><code>max_workers</code> (int, optional): Maximum number of parallel workers. Defaults to global configuration.</li> <li><code>batch_size</code> (int, optional): Number of items to process in each batch. Defaults to global configuration.</li> <li><code>rate_limit</code>: Rate limiting configuration. Can be:</li> <li>A float (operations per second)</li> <li>A tuple of (count, unit) where unit is \"second\", \"minute\", or \"hour\"</li> <li>A RateLimit instance</li> <li><code>executor_type</code> (str, optional): Type of parallelism to use (\"thread\" or \"process\"). Defaults to global configuration.</li> <li><code>prewarm</code> (bool): If True, starts all workers immediately.</li> </ul>"},{"location":"api-reference/decorators/#returns","title":"Returns","text":"<p>A wrapped function that accepts either: - A single item (returns a single-item list) - A list/tuple of items (processes in parallel and returns a list of results)</p>"},{"location":"api-reference/decorators/#examples","title":"Examples","text":"<pre><code># Basic usage with threads\n@parallel(max_workers=4)\ndef fetch_url(url: str) -&gt; dict:\n    return requests.get(url).json()\n\n# Process-based execution with rate limiting\n@parallel(\n    max_workers=4,\n    executor_type=\"process\",\n    rate_limit=(100, \"minute\")\n)\ndef process_image(image: bytes) -&gt; bytes:\n    return heavy_processing(image)\n\n# Batch processing with prewarming\n@parallel(\n    max_workers=4,\n    batch_size=10,\n    prewarm=True\n)\ndef analyze_text(text: str) -&gt; dict:\n    return text_analysis(text)\n</code></pre>"},{"location":"api-reference/rate-limiting/","title":"API Reference","text":""},{"location":"api-reference/rate-limiting/#rate-limiting-api","title":"Rate Limiting API","text":""},{"location":"api-reference/rate-limiting/#ratelimit","title":"RateLimit","text":"<p>Configuration class for rate limiting parallel operations.</p> <pre><code>from pyarallel import RateLimit\n\nrate = RateLimit(\n    count: float,\n    interval: Literal[\"second\", \"minute\", \"hour\"] = \"second\"\n)\n</code></pre>"},{"location":"api-reference/rate-limiting/#parameters","title":"Parameters","text":"<ul> <li><code>count</code> (float): Number of operations allowed per interval</li> <li><code>interval</code> (str): Time interval for rate limiting (\"second\", \"minute\", \"hour\")</li> </ul>"},{"location":"api-reference/rate-limiting/#properties","title":"Properties","text":"<ul> <li><code>per_second</code> (float): Converts the rate to operations per second</li> </ul>"},{"location":"api-reference/rate-limiting/#examples","title":"Examples","text":"<pre><code># Define rate limits\nper_minute = RateLimit(100, \"minute\")  # 100 ops/minute\nper_hour = RateLimit(1000, \"hour\")    # 1000 ops/hour\n\n# Use with parallel decorator\n@parallel(rate_limit=per_minute)\ndef rate_limited_function(item): ...\n</code></pre>"},{"location":"api-reference/rate-limiting/#tokenbucket","title":"TokenBucket","text":"<p>Thread-safe implementation of the token bucket algorithm for rate limiting.</p> <pre><code>from pyarallel import TokenBucket\n\nbucket = TokenBucket(\n    rate_limit: RateLimit,\n    capacity: int = None\n)\n</code></pre>"},{"location":"api-reference/rate-limiting/#parameters_1","title":"Parameters","text":"<ul> <li><code>rate_limit</code> (RateLimit): Rate limiting configuration</li> <li><code>capacity</code> (int, optional): Maximum number of tokens the bucket can hold</li> </ul>"},{"location":"api-reference/rate-limiting/#methods","title":"Methods","text":"<ul> <li><code>get_token() -&gt; bool</code>: Try to get a token from the bucket</li> <li><code>wait_for_token()</code>: Block until a token is available</li> </ul>"},{"location":"development/CONTRIBUTING/","title":"Contributing to Pyarallel","text":"<p>We love your input! We want to make contributing to Pyarallel as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul>"},{"location":"development/CONTRIBUTING/#development-process","title":"Development Process","text":"<p>We use GitHub to host code, to track issues and feature requests, as well as accept pull requests.</p> <ol> <li>Fork the repo and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"development/CONTRIBUTING/#any-contributions-you-make-will-be-under-the-mit-license","title":"Any Contributions You Make Will Be Under the MIT License","text":"<p>In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"development/CONTRIBUTING/#report-bugs-using-githubs-issue-tracker","title":"Report Bugs Using GitHub's Issue Tracker","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue; it's that easy!</p>"},{"location":"development/CONTRIBUTING/#write-bug-reports-with-detail-background-and-sample-code","title":"Write Bug Reports With Detail, Background, and Sample Code","text":"<p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce</li> <li>Be specific!</li> <li>Give sample code if you can.</li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul>"},{"location":"development/CONTRIBUTING/#use-a-consistent-coding-style","title":"Use a Consistent Coding Style","text":"<ul> <li>Use Black for Python code formatting</li> <li>Keep line length to 88 characters (Black default)</li> <li>Use type hints for function arguments and return values</li> <li>Write docstrings for all public functions and classes</li> </ul>"},{"location":"development/CONTRIBUTING/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"development/CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under its MIT License.</p>"},{"location":"development/roadmap/","title":"Roadmap","text":""},{"location":"development/roadmap/#roadmap","title":"Roadmap","text":"<p>Some of the features we're planning to add in the future:</p>"},{"location":"development/roadmap/#observability-debugging","title":"Observability &amp; Debugging","text":"<ul> <li>Advanced Telemetry System</li> <li>Task execution metrics (duration, wait times, queue times)</li> <li>Worker utilization tracking</li> <li>Error frequency analysis</li> <li>SQLite persistence for historical data</li> <li>Interactive visualizations with Plotly</li> <li>Performance bottleneck identification</li> </ul> <ul> <li>Rich Logging System</li> <li>Configurable log levels per component</li> <li>Structured logging for machine parsing</li> <li>Contextual information for debugging</li> <li>Log rotation and management</li> <li>Integration with popular logging frameworks</li> </ul>"},{"location":"development/roadmap/#advanced-features","title":"Advanced Features","text":"<ul> <li>Callback System</li> <li>Pre/post execution hooks</li> <li>Error handling callbacks</li> <li>Progress tracking</li> <li>Custom metrics collection</li> <li>State management hooks</li> </ul> <ul> <li>Smart Scheduling</li> <li>Priority queues for tasks</li> <li>Deadline-aware scheduling</li> <li>Resource-aware task distribution</li> <li>Adaptive batch sizing</li> <li>Dynamic worker scaling</li> </ul> <ul> <li>Fault Tolerance</li> <li>Automatic retries with backoff</li> <li>Circuit breaker pattern</li> <li>Fallback strategies</li> <li>Dead letter queues</li> <li>Task timeout handling</li> </ul> <ul> <li>Resource Management</li> <li>Memory usage monitoring</li> <li>CPU utilization tracking</li> <li>Network bandwidth control</li> <li>Disk I/O rate limiting</li> <li>Resource quotas per task</li> </ul>"},{"location":"development/roadmap/#developer-experience","title":"Developer Experience","text":"<ul> <li>CLI Tools</li> <li>Task monitoring dashboard</li> <li>Performance profiling</li> <li>Configuration management</li> <li>Log analysis utilities</li> <li>Telemetry visualization</li> </ul>"},{"location":"development/roadmap/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Integration</li> <li>Distributed tracing (OpenTelemetry)</li> <li>Metrics export (Prometheus)</li> <li>Log aggregation (ELK Stack)</li> </ul> <p>If you have any ideas or suggestions, feel free to open an issue or submit a pull request!</p> <p>Want to contribute? Check out our CONTRIBUTING.md guide!</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install Pyarallel using pip:</p> <pre><code>pip install pyarallel\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or contributing to Pyarallel:</p> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/oneryalcin/pyarallel.git\ncd pyarallel\n</code></pre></p> </li> <li> <p>Install development dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Verify your installation by running Python and importing Pyarallel:</p> <pre><code>from pyarallel import parallel\n\n# Test with a simple parallel function\n@parallel(max_workers=2)\ndef test_func(x):\n    return x * 2\n\nresult = test_func([1, 2, 3])\nprint(result)  # Should print: [2, 4, 6]\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation:</p> <ol> <li> <p>Ensure you have the latest pip version: <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Check Python version compatibility: <pre><code>python --version\n</code></pre></p> </li> <li> <p>For platform-specific issues or additional help, please refer to our GitHub Issues page.</p> </li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":"<p>Pyarallel makes parallel processing in Python simple and efficient. Here's how to get started:</p> <pre><code>from pyarallel import parallel\n\n# Basic parallel processing with threads\n@parallel(max_workers=4)\ndef fetch_url(url: str) -&gt; dict:\n    import requests\n    return requests.get(url).json()\n\n# Process multiple URLs in parallel\nurls = [\n    \"https://api.example.com/data/1\",\n    \"https://api.example.com/data/2\"\n]\nresults = fetch_url(urls)  # Returns list of results\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#cpu-bound-tasks","title":"CPU-Bound Tasks","text":"<p>For CPU-intensive operations, use process-based parallelism:</p> <pre><code>@parallel(\n    max_workers=4,\n    executor_type=\"process\"  # Use processes instead of threads\n)\ndef process_data(data: bytes) -&gt; bytes:\n    # CPU-intensive computation\n    return heavy_computation(data)\n</code></pre>"},{"location":"getting-started/quickstart/#rate-limited-operations","title":"Rate-Limited Operations","text":"<p>Control execution rates for API calls or resource-intensive operations:</p> <pre><code>@parallel(\n    max_workers=4,\n    rate_limit=(100, \"minute\")  # 100 operations per minute\n)\ndef api_call(item_id: str) -&gt; dict:\n    return api.get_item(item_id)\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<p>Handle large datasets efficiently with automatic batching:</p> <pre><code>@parallel(\n    max_workers=4,\n    batch_size=10  # Process items in batches of 10\n)\ndef analyze_text(text: str) -&gt; dict:\n    return text_analysis(text)\n\n# Process a large list of texts\ntexts = [\"text1\", \"text2\", ..., \"text1000\"]\nresults = analyze_text(texts)  # Processed in batches\n</code></pre>"},{"location":"getting-started/quickstart/#error-handling","title":"Error Handling","text":"<p>Pyarallel provides comprehensive error handling:</p> <pre><code>@parallel(max_workers=4)\ndef process_item(item):\n    try:\n        return do_work(item)\n    except Exception as e:\n        # Errors are propagated to the caller\n        raise RuntimeError(f\"Failed to process {item}: {e}\")\n\n# Handle errors in the caller\ntry:\n    results = process_item(items)\nexcept Exception as e:\n    print(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Advanced Features for more capabilities</li> <li>Check Configuration for customization options</li> <li>Review Best Practices for optimization tips</li> </ul>"},{"location":"user-guide/advanced-features/","title":"Advanced Features","text":""},{"location":"user-guide/advanced-features/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/advanced-features/#executor-types","title":"Executor Types","text":"<p>Pyarallel supports both thread and process-based executors:</p> <pre><code># Thread-based for I/O-bound tasks (default)\n@parallel(executor_type=\"thread\")\ndef io_task(): ...\n\n# Process-based for CPU-bound tasks\n@parallel(executor_type=\"process\")\ndef cpu_task(): ...\n</code></pre>"},{"location":"user-guide/advanced-features/#worker-management","title":"Worker Management","text":""},{"location":"user-guide/advanced-features/#worker-prewarming","title":"Worker Prewarming","text":"<p>Reduce initial latency by prewarming workers:</p> <pre><code>@parallel(\n    max_workers=4,\n    prewarm=True  # Start workers immediately\n)\ndef latency_sensitive_task(): ...\n</code></pre>"},{"location":"user-guide/advanced-features/#dynamic-worker-pools","title":"Dynamic Worker Pools","text":"<p>Pyarallel automatically manages worker lifecycles:</p> <pre><code># Workers are reused across calls\n@parallel(max_workers=4)\ndef reused_pool_task(): ...\n\n# Workers are cleaned up when no longer needed\n</code></pre>"},{"location":"user-guide/advanced-features/#rate-limiting","title":"Rate Limiting","text":""},{"location":"user-guide/advanced-features/#time-based-rate-limits","title":"Time-Based Rate Limits","text":"<pre><code># Operations per second\n@parallel(rate_limit=10)  # 10 ops/second\n\n# Operations per minute\n@parallel(rate_limit=(100, \"minute\"))  # 100 ops/minute\n\n# Operations per hour\n@parallel(rate_limit=(1000, \"hour\"))  # 1000 ops/hour\n</code></pre>"},{"location":"user-guide/advanced-features/#custom-rate-limiting","title":"Custom Rate Limiting","text":"<pre><code>from pyarallel import RateLimit\n\n# Create custom rate limit\nrate = RateLimit(count=50, interval=\"minute\")\n\n@parallel(rate_limit=rate)\ndef rate_limited_task(): ...\n</code></pre>"},{"location":"user-guide/advanced-features/#batch-processing","title":"Batch Processing","text":""},{"location":"user-guide/advanced-features/#automatic-batching","title":"Automatic Batching","text":"<pre><code>@parallel(\n    max_workers=4,\n    batch_size=10,  # Process 10 items at a time\n    executor_type=\"process\"\n)\ndef process_batch(items: list) -&gt; list:\n    return [process_item(item) for item in items]\n\n# Process large dataset\nitems = list(range(1000))\nresults = process_batch(items)  # Processed in batches of 10\n</code></pre>"},{"location":"user-guide/advanced-features/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code>@parallel(\n    max_workers=4,\n    batch_size=100  # Larger batches for better throughput\n)\ndef process_large_dataset(data: list) -&gt; list:\n    return heavy_processing(data)\n\n# Process millions of items efficiently\nresults = process_large_dataset(large_dataset)\n</code></pre>"},{"location":"user-guide/advanced-features/#advanced-error-handling","title":"Advanced Error Handling","text":""},{"location":"user-guide/advanced-features/#batch-error-handling","title":"Batch Error Handling","text":"<pre><code>from typing import List, Optional\n\n@parallel(batch_size=10)\ndef process_with_errors(items: List[str]) -&gt; List[Optional[dict]]:\n    results = []\n    for item in items:\n        try:\n            results.append(process_item(item))\n        except Exception as e:\n            results.append(None)  # Continue on error\n            print(f\"Error processing {item}: {e}\")\n    return results\n</code></pre>"},{"location":"user-guide/advanced-features/#custom-exception-handling","title":"Custom Exception Handling","text":"<pre><code>class ProcessingError(Exception):\n    pass\n\n@parallel(max_workers=4)\ndef safe_process(item):\n    try:\n        result = process_item(item)\n        if not validate_result(result):\n            raise ProcessingError(f\"Invalid result for {item}\")\n        return result\n    except ProcessingError as e:\n        # Handle specific errors\n        handle_processing_error(e)\n    except Exception as e:\n        # Handle unexpected errors\n        handle_unexpected_error(e)\n</code></pre>"},{"location":"user-guide/advanced-features/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/advanced-features/#worker-pool-optimization","title":"Worker Pool Optimization","text":"<pre><code># Optimize for CPU-bound tasks\n@parallel(\n    max_workers=multiprocessing.cpu_count(),\n    executor_type=\"process\"\n)\ndef cpu_optimized_task(): ...\n\n# Optimize for I/O-bound tasks\n@parallel(\n    max_workers=32,  # Higher worker count for I/O\n    executor_type=\"thread\"\n)\ndef io_optimized_task(): ...\n</code></pre>"},{"location":"user-guide/advanced-features/#batch-size-optimization","title":"Batch Size Optimization","text":"<pre><code># Small batches for low latency\n@parallel(batch_size=5)\ndef low_latency_task(): ...\n\n# Large batches for high throughput\n@parallel(batch_size=100)\ndef high_throughput_task(): ...\n</code></pre>"},{"location":"user-guide/best-practices/","title":"Best Practices","text":""},{"location":"user-guide/best-practices/#choosing-the-right-executor","title":"Choosing the Right Executor","text":""},{"location":"user-guide/best-practices/#thread-vs-process","title":"Thread vs Process","text":"<ul> <li>Use threads for I/O-bound tasks:</li> <li>Network requests</li> <li>File operations</li> <li>Database queries</li> </ul> <ul> <li>Use processes for CPU-bound tasks:</li> <li>Data processing</li> <li>Image manipulation</li> <li>Complex calculations</li> </ul> <pre><code># I/O-bound example\n@parallel(executor_type=\"thread\")\ndef fetch_data(urls: list) -&gt; list:\n    return [requests.get(url).json() for url in urls]\n\n# CPU-bound example\n@parallel(executor_type=\"process\")\ndef process_images(images: list) -&gt; list:\n    return [heavy_image_processing(img) for img in images]\n</code></pre>"},{"location":"user-guide/best-practices/#worker-pool-management","title":"Worker Pool Management","text":""},{"location":"user-guide/best-practices/#optimal-worker-count","title":"Optimal Worker Count","text":"<p>For CPU-bound tasks:</p> <ul> <li>Use <code>multiprocessing.cpu_count()</code> as a baseline</li> <li>Consider leaving 1-2 cores free for system tasks</li> </ul> <p>For I/O-bound tasks:</p> <ul> <li>Can use more workers than CPU cores</li> <li>Monitor system resources to find optimal number</li> </ul> <pre><code>import multiprocessing\n\n# CPU-bound optimization\n@parallel(\n    max_workers=max(1, multiprocessing.cpu_count() - 1),\n    executor_type=\"process\"\n)\ndef cpu_intensive_task(): ...\n\n# I/O-bound optimization\n@parallel(\n    max_workers=32,  # Higher count for I/O tasks\n    executor_type=\"thread\"\n)\ndef io_intensive_task(): ...\n</code></pre>"},{"location":"user-guide/best-practices/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/best-practices/#batch-processing","title":"Batch Processing","text":"<ul> <li>Use batching for large datasets</li> <li>Adjust batch size based on memory constraints</li> <li>Monitor memory usage during processing</li> </ul> <pre><code># Memory-efficient processing\n@parallel(\n    max_workers=4,\n    batch_size=100,  # Adjust based on item size\n    executor_type=\"process\"\n)\ndef process_large_dataset(items: list) -&gt; list:\n    return [process_item(item) for item in items]\n</code></pre>"},{"location":"user-guide/best-practices/#resource-cleanup","title":"Resource Cleanup","text":"<ul> <li>Let Pyarallel handle worker lifecycle</li> <li>Avoid manual executor management</li> <li>Use context managers when needed</li> </ul>"},{"location":"user-guide/best-practices/#rate-limiting","title":"Rate Limiting","text":""},{"location":"user-guide/best-practices/#api-considerations","title":"API Considerations","text":"<ul> <li>Respect API rate limits</li> <li>Add buffer to prevent limit breaches</li> <li>Use appropriate time intervals</li> </ul> <pre><code># Safe API usage\n@parallel(\n    max_workers=4,\n    rate_limit=(90, \"minute\")  # 90% of 100/minute limit\n)\ndef api_call(item_id: str) -&gt; dict:\n    return api.get_item(item_id)\n</code></pre>"},{"location":"user-guide/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/best-practices/#graceful-failure","title":"Graceful Failure","text":"<ul> <li>Implement proper error handling</li> <li>Log errors for debugging</li> <li>Consider retry mechanisms</li> </ul> <pre><code>import logging\nfrom tenacity import retry, stop_after_attempt\n\n@retry(stop=stop_after_attempt(3))\n@parallel(max_workers=4)\ndef resilient_process(item):\n    try:\n        result = process_item(item)\n        if not validate_result(result):\n            raise ValueError(f\"Invalid result for {item}\")\n        return result\n    except Exception as e:\n        logging.error(f\"Error processing {item}: {e}\")\n        raise\n</code></pre>"},{"location":"user-guide/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/best-practices/#prewarming","title":"Prewarming","text":"<ul> <li>Use prewarming for latency-sensitive applications</li> <li>Consider startup cost vs benefit</li> </ul> <pre><code>@parallel(\n    max_workers=4,\n    prewarm=True,  # Prewarm for faster initial response\n    executor_type=\"process\"\n)\ndef latency_sensitive_task(): ...\n</code></pre>"},{"location":"user-guide/best-practices/#batch-size-optimization","title":"Batch Size Optimization","text":"<ul> <li>Small batches for low latency</li> <li>Larger batches for high throughput</li> <li>Balance based on use case</li> </ul> <pre><code># Low latency needs\n@parallel(batch_size=5)\ndef realtime_processing(): ...\n\n# High throughput needs\n@parallel(batch_size=100)\ndef bulk_processing(): ...\n</code></pre>"},{"location":"user-guide/best-practices/#testing-and-monitoring","title":"Testing and Monitoring","text":""},{"location":"user-guide/best-practices/#unit-testing","title":"Unit Testing","text":"<ul> <li>Test with different worker counts</li> <li>Verify error handling</li> <li>Check resource cleanup</li> </ul> <pre><code>def test_parallel_processing():\n    @parallel(max_workers=2)\n    def test_func(x):\n        return x * 2\n\n    # Test with various inputs\n    assert test_func([1, 2, 3]) == [2, 4, 6]\n\n    # Test error handling\n    with pytest.raises(ValueError):\n        test_func(['invalid'])\n</code></pre>"},{"location":"user-guide/best-practices/#production-monitoring","title":"Production Monitoring","text":"<ul> <li>Monitor worker pool health</li> <li>Track memory usage</li> <li>Log performance metrics</li> </ul> <pre><code>import logging\n\n@parallel(max_workers=4)\ndef monitored_task(item):\n    start_time = time.time()\n    try:\n        result = process_item(item)\n        duration = time.time() - start_time\n        logging.info(f\"Processed {item} in {duration:.2f}s\")\n        return result\n    except Exception as e:\n        logging.error(f\"Failed to process {item}: {e}\")\n        raise\n</code></pre>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>Pyarallel features a robust configuration system built on Pydantic, offering type validation, environment variable support, and thread-safe configuration management.</p>"},{"location":"user-guide/configuration/#configuration-schema","title":"Configuration Schema","text":"<p>The configuration system uses a structured schema with the following categories:</p> <pre><code>{\n    \"execution\": {\n        \"default_max_workers\": int,        # Default worker count\n        \"default_executor_type\": str,      # \"thread\" or \"process\"\n        \"default_batch_size\": Optional[int], # Default batch size\n        \"prewarm_pools\": bool             # Enable worker prewarming\n    },\n    \"rate_limiting\": {\n        \"default_rate\": Optional[float],   # Default operations per interval\n        \"default_interval\": str,          # \"second\", \"minute\", \"hour\"\n        \"burst_tolerance\": float          # Burst allowance factor\n    },\n    \"error_handling\": {\n        \"max_retries\": int,               # Maximum retry attempts\n        \"retry_backoff\": float,           # Backoff multiplier\n        \"fail_fast\": bool                 # Stop on first error\n    },\n    \"monitoring\": {\n        \"enable_logging\": bool,           # Enable detailed logging\n        \"log_level\": str,                # Logging level\n        \"sentry_dsn\": Optional[str],     # Sentry integration\n        \"metrics_enabled\": bool          # Enable metrics collection\n    }\n}\n</code></pre>"},{"location":"user-guide/configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from pyarallel import ConfigManager\n\n# Get the thread-safe singleton configuration manager\nconfig = ConfigManager.get_instance()\n\n# Update configuration with type validation\nconfig.update_config({\n    \"execution\": {\n        \"default_max_workers\": 8,\n        \"default_executor_type\": \"thread\",\n        \"default_batch_size\": 100,\n        \"prewarm_pools\": True\n    },\n    \"rate_limiting\": {\n        \"default_rate\": 1000,\n        \"default_interval\": \"minute\",\n        \"burst_tolerance\": 1.5\n    }\n})\n\n# Access configuration using dot notation\nworkers = config.execution.default_max_workers\nrate = config.rate_limiting.default_rate\n\n# Category-specific updates\nconfig.update_execution(max_workers=16)\nconfig.update_rate_limiting(rate=2000)\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure Pyarallel using environment variables with the <code>PYARALLEL_</code> prefix. The system automatically handles type coercion and validation:</p> <pre><code># Execution settings\nexport PYARALLEL_MAX_WORKERS=4\nexport PYARALLEL_EXECUTOR_TYPE=thread\nexport PYARALLEL_BATCH_SIZE=100\n\n# Rate limiting\nexport PYARALLEL_RATE_LIMIT=100/minute\nexport PYARALLEL_FAIL_FAST=true\n\n# Complex values (using JSON)\nexport PYARALLEL_RETRY_CONFIG='{\"max_attempts\": 3, \"backoff\": 1.5}'\n</code></pre>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use Environment Variables for Deployment:</li> <li>Keep configuration in environment variables for different environments</li> <li>Use the <code>PYARALLEL_</code> prefix to avoid conflicts</li> <li> <p>Complex values can be passed as JSON strings</p> </li> <li> <p>Validate Configuration Early:</p> </li> <li>Set up configuration at application startup</li> <li>Use type validation to catch issues early</li> <li> <p>Test configuration with sample data</p> </li> <li> <p>Thread-Safe Updates:</p> </li> <li>Always use <code>ConfigManager.get_instance()</code> for thread-safe access</li> <li>Make configuration changes before starting parallel operations</li> <li> <p>Use category-specific update methods for better type safety</p> </li> <li> <p>Configuration Inheritance:</p> </li> <li>Global settings serve as defaults</li> <li>Decorator arguments override global configuration</li> <li>Environment variables take precedence over code-based configuration</li> </ol>"},{"location":"user-guide/configuration/#runtime-configuration-warnings","title":"Runtime Configuration Warnings","text":"<p>Pyarallel includes built-in warnings to help identify potential performance issues:</p> <pre><code># Warning for high worker count\n@parallel(max_workers=150)  # Triggers warning about system impact\ndef high_worker_task(): ...\n\n# Warning for inefficient process pool configuration\n@parallel(\n    executor_type=\"process\",\n    batch_size=1  # Triggers warning about inefficient batch size\n)\ndef inefficient_task(): ...\n</code></pre>"},{"location":"user-guide/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>Pyarallel uses a hierarchical configuration system:</p> <ol> <li>Default Values: Built-in defaults (4 workers, thread executor, batch size 10)</li> <li>Global Configuration: Set via ConfigManager</li> <li>Environment Variables: Override global config</li> <li>Decorator Arguments: Highest precedence, override all other settings</li> </ol> <pre><code># Global configuration (lowest precedence)\nconfig = ConfigManager.get_instance()\nconfig.update_config({\n    \"execution\": {\n        \"default_max_workers\": 8,\n        \"default_executor_type\": \"thread\"\n    }\n})\n\n# Environment variables (middle precedence)\n# export PYARALLEL_MAX_WORKERS=16\n\n# Decorator arguments (highest precedence)\n@parallel(max_workers=4)  # This value wins\ndef my_func(): ...\n</code></pre>"}]}